{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.spatial import KDTree\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd.variable import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from geomloss import SamplesLoss\n",
    "import os.path as osp\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import ModelNet\n",
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "from IPython import embed\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_dist = []\n",
    "# for i in range(100): \n",
    "#     x = torch.rand(500,1)\n",
    "#     set_dist.append(x)\n",
    "    \n",
    "# for i in range(35): \n",
    "#     m = torch.distributions.beta.Beta(torch.tensor([.5]), torch.tensor([.5]))\n",
    "#     x = m.sample([500])\n",
    "#     set_dist.append(x)\n",
    "    \n",
    "# for i in range(35): \n",
    "#     m = torch.distributions.beta.Beta(torch.tensor([.7]), torch.tensor([.3]))\n",
    "#     x = m.sample([500])\n",
    "#     set_dist.append(x)\n",
    "    \n",
    "# for i in range(40): \n",
    "#     m = torch.distributions.beta.Beta(torch.tensor([.2]), torch.tensor([.7]))\n",
    "#     x = m.sample([500])\n",
    "#     set_dist.append(x)\n",
    "    \n",
    "# for i in range(50): \n",
    "#     m = torch.distributions.exponential.Exponential(torch.tensor([1.5]))\n",
    "#     x = m.sample([500])\n",
    "#     set_dist.append(x)\n",
    "    \n",
    "# for i in range(50): \n",
    "#     m = torch.distributions.exponential.Exponential(torch.tensor([2.5]))\n",
    "#     x = m.sample([500])\n",
    "#     set_dist.append(x)\n",
    "    \n",
    "# for i in range(100): \n",
    "#     m = torch.distributions.gamma.Gamma(torch.tensor([1.0]), torch.tensor([1.5]))\n",
    "#     x = m.sample([500])\n",
    "#     set_dist.append(x)\n",
    "    \n",
    "# for i in range(40): \n",
    "#     m = torch.distributions.laplace.Laplace(torch.tensor([1.0]), torch.tensor([1.5]))\n",
    "#     x = m.sample([500])\n",
    "#     set_dist.append(x)\n",
    "    \n",
    "# for i in range(59): \n",
    "#     m = torch.distributions.laplace.Laplace(torch.tensor([.5]), torch.tensor([1.0]))\n",
    "#     x = m.sample([500])\n",
    "#     set_dist.append(x)\n",
    "    \n",
    "# for i in range(100): \n",
    "#     m = torch.distributions.log_normal.LogNormal(torch.tensor([0.0]), torch.tensor([0.5]))\n",
    "#     x = m.sample([500])\n",
    "#     set_dist.append(x)   \n",
    "\n",
    "    \n",
    "# for i in range(59): \n",
    "#     m = torch.distributions.normal.Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "#     x = m.sample([500])\n",
    "#     set_dist.append(x)   \n",
    "\n",
    "# for i in range(50): \n",
    "#     m = torch.distributions.normal.Normal(torch.tensor([0.3]), torch.tensor([0.5]))\n",
    "#     x = m.sample([500])\n",
    "#     set_dist.append(x)   \n",
    "    \n",
    "    \n",
    "# for i in range(100): \n",
    "#     m = torch.distributions.studentT.StudentT(torch.tensor([2.0]))\n",
    "#     x = m.sample([500])\n",
    "#     set_dist.append(x)      \n",
    "    \n",
    "# set_dist = torch.stack(set_dist)\n",
    "# set_dist.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = osp.join('..', 'data/ModelNet10')\n",
    "set_size = 100\n",
    "batch_size = 16\n",
    "data_ratio = 1\n",
    "pre_transform, transform = T.NormalizeScale(), T.SamplePoints(set_size)\n",
    "train_dataset = ModelNet(path, '10', True, transform, pre_transform)\n",
    "data_size = train_dataset.__len__()\n",
    "perm = torch.randperm(data_size)\n",
    "idx = perm[:int(data_size*data_ratio)]\n",
    "train_dataset = train_dataset.__getitem__(idx)\n",
    "test_dataset = ModelNet(path, '10', False, transform, pre_transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=6)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=6)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.empty([len(train_loader.dataset), set_size, 3])\n",
    "# Y = np.empty(len(train_loader.dataset), dtype=int)\n",
    "    \n",
    "# for i, batch in enumerate(train_loader):\n",
    "#     n_data = batch.pos.reshape(-1, set_size, 3).to(device)\n",
    "#     X[i * batch_size:(i + 1) * batch_size] = n_data.detach().cpu().numpy()\n",
    "#     Y[i * batch_size:(i + 1) * batch_size] = batch.y.cpu().numpy()\n",
    "\n",
    "# print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SinkhornDistance(nn.Module):\n",
    "#     r\"\"\"\n",
    "#     Given two empirical measures each with :math:`P_1` locations\n",
    "#     :math:`x\\in\\mathbb{R}^{D_1}` and :math:`P_2` locations :math:`y\\in\\mathbb{R}^{D_2}`,\n",
    "#     outputs an approximation of the regularized OT cost for point clouds.\n",
    "#     Args:\n",
    "#         eps (float): regularization coefficient\n",
    "#         max_iter (int): maximum number of Sinkhorn iterations\n",
    "#         reduction (string, optional): Specifies the reduction to apply to the output:\n",
    "#             'none' | 'mean' | 'sum'. 'none': no reduction will be applied,\n",
    "#             'mean': the sum of the output will be divided by the number of\n",
    "#             elements in the output, 'sum': the output will be summed. Default: 'none'\n",
    "#     Shape:\n",
    "#         - Input: :math:`(N, P_1, D_1)`, :math:`(N, P_2, D_2)`\n",
    "#         - Output: :math:`(N)` or :math:`()`, depending on `reduction`\n",
    "#     \"\"\"\n",
    "#     def __init__(self, eps, max_iter, reduction='none'):\n",
    "#         super(SinkhornDistance, self).__init__()\n",
    "#         self.eps = eps\n",
    "#         self.max_iter = max_iter\n",
    "#         self.reduction = reduction\n",
    "# #         if device is None:\n",
    "# #             self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# #         else:\n",
    "# #             self.device = device\n",
    "# #         self.to(self.device)\n",
    "\n",
    "#     def forward(self, x, y):\n",
    "#         # The Sinkhorn algorithm takes as input three variables :\n",
    "#         C = self._cost_matrix(x, y)  # Wasserstein cost function\n",
    "#         x_points = x.shape[-2]\n",
    "#         y_points = y.shape[-2]\n",
    "#         if x.dim() == 2:\n",
    "#             batch_size = 1\n",
    "#         else:\n",
    "#             batch_size = x.shape[0]\n",
    "\n",
    "#         # both marginals are fixed with equal weights\n",
    "#         mu = torch.empty(batch_size, x_points, dtype=torch.float,\n",
    "#                          requires_grad=False).fill_(1.0 / x_points).to(device).squeeze()\n",
    "#         nu = torch.empty(batch_size, y_points, dtype=torch.float,\n",
    "#                          requires_grad=False).fill_(1.0 / y_points).to(device).squeeze()\n",
    "\n",
    "#         u = torch.zeros_like(mu).to(device)\n",
    "#         v = torch.zeros_like(nu).to(device)\n",
    "#         # To check if algorithm terminates because of threshold\n",
    "#         # or max iterations reached\n",
    "#         actual_nits = 0\n",
    "#         # Stopping criterion\n",
    "#         thresh = 1e-1\n",
    "\n",
    "#         # Sinkhorn iterations\n",
    "#         for i in range(self.max_iter):\n",
    "#             u1 = u  # useful to check the update\n",
    "#             u = self.eps * (torch.log(mu+1e-8) - torch.logsumexp(self.M(C, u, v), dim=-1)) + u\n",
    "#             v = self.eps * (torch.log(nu+1e-8) - torch.logsumexp(self.M(C, u, v).transpose(-2, -1), dim=-1)) + v\n",
    "#             err = (u - u1).abs().sum(-1).mean()\n",
    "\n",
    "#             actual_nits += 1\n",
    "#             if err.item() < thresh:\n",
    "#                 break\n",
    "\n",
    "#         U, V = u, v\n",
    "#         # Transport plan pi = diag(a)*K*diag(b)\n",
    "#         pi = torch.exp(self.M(C, U, V))\n",
    "#         # Sinkhorn distance\n",
    "#         cost = torch.sum(pi * C, dim=(-2, -1))\n",
    "\n",
    "#         if self.reduction == 'mean':\n",
    "#             cost = cost.mean()\n",
    "#         elif self.reduction == 'sum':\n",
    "#             cost = cost.sum()\n",
    "\n",
    "#       #  return cost, pi, C\n",
    "#         return cost\n",
    "\n",
    "#     def M(self, C, u, v):\n",
    "#         \"Modified cost for logarithmic updates\"\n",
    "#         \"$M_{ij} = (-c_{ij} + u_i + v_j) / \\epsilon$\"\n",
    "#         return (-C + u.unsqueeze(-1) + v.unsqueeze(-2)) / self.eps\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _cost_matrix(x, y, p=2):\n",
    "#         \"Returns the matrix of $|x_i-y_j|^p$.\"\n",
    "#         x_col = x.unsqueeze(-2)\n",
    "#         y_lin = y.unsqueeze(-3)\n",
    "#         C = torch.sum((torch.abs(x_col - y_lin)) ** p, -1)\n",
    "#         return C\n",
    "\n",
    "#     @staticmethod\n",
    "#     def ave(u, u1, tau):\n",
    "#         \"Barycenter subroutine, used by kinetic acceleration through extrapolation.\"\n",
    "#         return tau * u + (1 - tau) * u1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_sinkhorn_distances(data, batch_size=20000, save_path=None, force_reload=False):\n",
    "#     if force_reload is False and save_path is not None and os.path.exists(save_path):\n",
    "#         distances = torch.load(save_path)\n",
    "#         return distances\n",
    "# #     sinkhorn = SinkhornDistance(eps=0.1, max_iter=100, device=device)\n",
    "#     sinkhorn = SinkhornDistance(eps=0.1, max_iter=100)\n",
    "# #     sinkhorn = SamplesLoss(loss=\"sinkhorn\", scaling=0.7)\n",
    "#     pairs = []\n",
    "#     N = data.shape[0]\n",
    "#     for i in tqdm(range(N)):\n",
    "#         for j in range(i+1, N):\n",
    "#             pairs.append(np.array([i,j]))\n",
    "#     pairs = np.stack(pairs)\n",
    "#     print(f'Len : {len(pairs)}')\n",
    "#     print('Pairing is done. Start calculations.')\n",
    "#     N_pairs = len(pairs)\n",
    "# #     distances = torch.zeros(N, N, dtype=torch.float, device=device)\n",
    "#     distances = torch.zeros(N, N, dtype=torch.double, device=device)\n",
    "#     for batch_id in tqdm(range(math.ceil(N_pairs / batch_size))):\n",
    "#         slice_ = slice(batch_id * batch_size, min(N_pairs, (batch_id+1) * batch_size))\n",
    "#         idx_0 = pairs[slice_,0]\n",
    "#         idx_1 = pairs[slice_,1]\n",
    "# #         embed()\n",
    "# #         slice_distances = sinkhorn(torch.tensor(data[idx_0,:,:], device=device), torch.tensor(data[idx_1,:,:], device=device)).detach()\n",
    "#         slice_distances = sinkhorn(data[idx_0,:,:], data[idx_1,:,:]).detach()\n",
    "#         distances[idx_0, idx_1] = slice_distances\n",
    "#         distances[idx_1, idx_0] = slice_distances\n",
    "#         # print('===pairs===')\n",
    "#         # print(slice_)\n",
    "#         # print(slice_distances.shape)\n",
    "#         # for i in range(slice_.start, slice_.stop):\n",
    "#         #   print(f'{pairs[i,:]} -> {slice_distances[i - slice_.start]}')\n",
    "#     if save_path is not None and os.path.exists(save_path):\n",
    "#         save_path.parent.mkdir(exist_ok=True)\n",
    "#         torch.save(distances, save_path)\n",
    "#     return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # out = get_sinkhorn_distances(set_dist.to(device), batch_size=1024)\n",
    "# out = get_sinkhorn_distances(torch.tensor(X, device=device), batch_size=512)\n",
    "# torch.save(out, \"distance_matrix_512_arijit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distances = torch.load(\"distance_matrix_512_arijit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from sklearn.manifold import TSNE\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# tsne = TSNE(n_components=2, init='random', metric='precomputed')\n",
    "# # tsne_embedding = tsne.fit_transform(out.cpu())\n",
    "# # plt.scatter(tsne_embedding[:,0], tsne_embedding[:,1], label='T-SNE')\n",
    "# # plt.legend()\n",
    "\n",
    "# X_2d = tsne.fit_transform(distances.cpu())\n",
    "# target_ids = range(10)\n",
    "# plt.figure(figsize=(6, 5))\n",
    "# colors = 'r', 'g', 'b', 'c', 'm', 'y', 'k', 'brown', 'orange', 'purple'\n",
    "# for i, c in zip(target_ids, colors):\n",
    "#     plt.scatter(X_2d[Y == i, 0], X_2d[Y == i, 1], c=c, label=i)\n",
    "# plt.legend()\n",
    "# # plt.savefig(\"tsne/test_%d.png\" %t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import umap\n",
    "# umap_embedder = umap.UMAP(metric='precomputed')\n",
    "# # umap_embedding = umap_embedder.fit_transform(out.cpu())\n",
    "# # plt.scatter(umap_embedding[:,0], umap_embedding[:,1], label='UMAP')\n",
    "# # plt.legend()\n",
    "\n",
    "# X_2d = umap_embedder.fit_transform(out.cpu())\n",
    "# target_ids = range(10)\n",
    "# plt.figure(figsize=(6, 5))\n",
    "# colors = 'r', 'g', 'b', 'c', 'm', 'y', 'k', 'brown', 'orange', 'purple'\n",
    "# for i, c in zip(target_ids, colors):\n",
    "#     plt.scatter(X_2d[Y == i, 0], X_2d[Y == i, 1], c=c, label=i)\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import phate\n",
    "\n",
    "# phate_operator = phate.PHATE(knn=5, decay=20, t=150, knn_dist='precomputed', n_jobs=-1)\n",
    "# # phate_embedding = phate_operator.fit_transform(out.cpu())\n",
    "# # plt.scatter(phate_embedding[:,0], phate_embedding[:,1], label='PHATE')\n",
    "# # plt.legend()\n",
    "# X_2d = phate_operator.fit_transform(out.cpu())\n",
    "# target_ids = range(10)\n",
    "# plt.figure(figsize=(6, 5))\n",
    "# colors = 'r', 'g', 'b', 'c', 'm', 'y', 'k', 'brown', 'orange', 'purple'\n",
    "# for i, c in zip(target_ids, colors):\n",
    "#     plt.scatter(X_2d[Y == i, 0], X_2d[Y == i, 1], c=c, label=i)\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSet(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, set_features):\n",
    "        super(DeepSet, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = set_features\n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(in_features, 50),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(50, 100),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(100, set_features),\n",
    "        )\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(set_features, 30),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(30, 30),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(30, 10),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Linear(10, 2),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = input\n",
    "        x = self.feature_extractor(x)\n",
    "        x = x.sum(dim=1)\n",
    "#         x = x.mean(dim=1)\n",
    "#         x = x.max(dim=1).values\n",
    "        x = self.regressor(x)\n",
    "        return x\n",
    "\n",
    "class PermEqui1_max(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(PermEqui1_max, self).__init__()\n",
    "        self.Gamma = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xm, _ = x.max(1, keepdim=True)\n",
    "        x = self.Gamma(x-xm)\n",
    "        return x\n",
    "    \n",
    "class DeepSet_pc(nn.Module):\n",
    "\n",
    "    def __init__(self, d_dim=256, x_dim=3):\n",
    "        super(DeepSet_pc, self).__init__()\n",
    "        self.d_dim = d_dim\n",
    "        self.x_dim = x_dim\n",
    "\n",
    "        self.phi = nn.Sequential(\n",
    "          PermEqui1_max(self.x_dim, self.d_dim),\n",
    "          nn.Tanh(),\n",
    "          PermEqui1_max(self.d_dim, self.d_dim),\n",
    "          nn.Tanh(),\n",
    "          PermEqui1_max(self.d_dim, self.d_dim),\n",
    "          nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.ro = nn.Sequential(\n",
    "           nn.Dropout(p=0.5),\n",
    "           nn.Linear(self.d_dim, self.d_dim),\n",
    "#            nn.Tanh(),\n",
    "#            nn.Dropout(p=0.5),\n",
    "#            nn.Linear(self.d_dim, 40),\n",
    "#             nn.Linear(self.d_dim, self.d_dim),\n",
    "        )\n",
    "        print(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        phi_output = self.phi(x)\n",
    "        sum_output, _ = phi_output.max(1)\n",
    "        ro_output = self.ro(sum_output)\n",
    "        return ro_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sinkhorn = SinkhornDistance(eps=0.1, max_iter=100, reduction=None).to(device)\n",
    "# sinkhorn = SinkhornDistance(eps=0.1, max_iter=100).to(device)\n",
    "# sinkhorn = SamplesLoss(loss=\"sinkhorn\", p=2, blur=.05)\n",
    "# sinkhorn = SamplesLoss(loss=\"sinkhorn\", scaling=0.7, debias=False)\n",
    "sinkhorn = SamplesLoss(loss=\"sinkhorn\", p=1, scaling=0.9, debias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MyDataset(Dataset):\n",
    "#     def __init__(self, data, transform=None):\n",
    "#         self.data = data.float()\n",
    "        \n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         x = self.data[index]\n",
    "        \n",
    "#         if self.transform:\n",
    "#             x = self.transform(x)\n",
    "           \n",
    "#         return x\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = MyDataset(set_dist)\n",
    "# train_loader = DataLoader(dataset, batch_size = 12, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DeepSet(1, 36).to(device)\n",
    "# model = DeepSet(3, 10).to(device)\n",
    "dim=256\n",
    "model = DeepSet_pc(d_dim=dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# checkpoint = torch.load('deepset_dist_2conditions_5.pt')\n",
    "# model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# loss = checkpoint['loss']\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "os.makedirs(\"tsne/\", exist_ok=True)\n",
    "tsne = TSNE(n_components=2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wasserstein distance has the following properties: \n",
    "1) W(aX,aY) = |a|W(X,Y)\n",
    "2) W(X+x, Y+x) = W(X,Y)\n",
    "3) ...\n",
    "Here we remove the 3rd condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "num_epochs = 5\n",
    "num_paths = 100\n",
    "running_loss = []\n",
    "total = 0\n",
    "total_time = []\n",
    "total_corr = []\n",
    "for t in range(num_epochs):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "    sink1 = []\n",
    "    euc = []\n",
    "    for n_batch, batch in enumerate(train_loader):\n",
    "        \n",
    "        batch = batch.pos.reshape(-1, set_size, 3)\n",
    "        n_data = Variable(batch.to(device), requires_grad=True)\n",
    "#         n_data = batch.to(device)\n",
    "        batch_size = n_data.shape[0]\n",
    "#         a = torch.rand(1).to(device)\n",
    "#         b = torch.rand(1).to(device)\n",
    "       \n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        y = model(n_data)\n",
    "#         y_a = model(a*n_data)\n",
    "#         y_translate = model(n_data + b)\n",
    "        \n",
    "#         loss = 0\n",
    "        src = np.random.randint(0, batch_size, size=num_paths)\n",
    "        dest = np.random.randint(0, batch_size, size=num_paths)\n",
    "        sink_dist = sinkhorn(n_data[src],n_data[dest])\n",
    "        sink_dist [src == dest] = 0\n",
    "        euc_dist = torch.norm(y[src]-y[dest], p=2, dim=1)\n",
    "        loss = torch.norm(sink_dist-euc_dist, p=2)/num_paths\n",
    "        \n",
    "#         for i in range(len(batch)):\n",
    "#             for j in range(i+1,len(batch)):\n",
    "#                 y_ij = torch.norm(y[i]-y[j], p=2)\n",
    "#                 w_ij = sinkhorn(n_data[i],n_data[j])\n",
    "                \n",
    "#                 ya_ij = torch.norm(y_a[i]-y_a[j], p=2)\n",
    "#                 y_translate_ij = torch.norm(y_translate[i]-y_translate[j], p=2)\n",
    "                \n",
    "#                 diff_translate_ij = torch.norm(y_translate[i]-y[j], p=2)**2\n",
    "                   \n",
    "#                 loss += torch.norm(y_ij-w_ij, p=2) + (ya_ij-a*y_ij)**2 + (y_translate_ij- y_ij)**2\n",
    "                \n",
    "#                 del w_ij\n",
    "            \n",
    "        #Trying without the last condition. That condition seems buggy\n",
    "        \n",
    "#         loss = calculate_loss(batch, n_data, a, y, y_a, y_translate)\n",
    "        \n",
    "        \n",
    "#         loss = loss/(len(batch)*(len(batch)-1)/2)\n",
    "       \n",
    "        loss.backward()\n",
    "       \n",
    "        optimizer.step()\n",
    "        \n",
    "#         print(\"Batch %d Done\", n_batch)\n",
    "        total_loss += loss\n",
    "    \n",
    "        sink1.extend(sink_dist[src != dest].cpu().detach().numpy())\n",
    "        euc.extend(euc_dist[src != dest].cpu().detach().numpy())\n",
    "#         cor, _ = pearsonr(sink_dist[src != dest].cpu().detach().numpy(),euc_dist[src != dest].cpu().detach().numpy())\n",
    "#         corr.append(cor)\n",
    "    \n",
    "        \n",
    "    running_loss.append(total_loss)\n",
    "    print(total_loss)\n",
    "    \n",
    "    end = time.time()\n",
    "    total += end-start\n",
    "    total_time.append(total)\n",
    "    \n",
    "    corr, _ = pearsonr(sink1,euc)\n",
    "    total_corr.append(corr)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    X = np.empty([len(train_loader.dataset), 256])\n",
    "    Y = np.empty(len(train_loader.dataset), dtype=int)\n",
    "\n",
    "    start = 0\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        n_data = batch.pos.reshape(-1, set_size, 3).to(device)\n",
    "        encodings = model(n_data)\n",
    "        batch_size = n_data.shape[0]\n",
    "        X[start:start+batch_size] = encodings.detach().cpu().numpy()\n",
    "        Y[start:start+batch_size] = batch.y.cpu().numpy()\n",
    "        start += batch_size\n",
    "\n",
    "    X_2d = tsne.fit_transform(X)\n",
    "    target_ids = range(10)\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    colors = 'r', 'g', 'b', 'c', 'm', 'y', 'k', 'brown', 'orange', 'purple'\n",
    "    for i, c, label in zip(target_ids, colors, target_ids):\n",
    "        plt.scatter(X_2d[Y == i, 0], X_2d[Y == i, 1], c=c, label=label)\n",
    "    plt.legend()\n",
    "    plt.savefig(\"tsne/test_%d.png\" %t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_time, total_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(running_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_loss.index(min(running_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First save is at 31 epochs, 2nd save after 31 epochs, 3rd save is after 41 epochs, \n",
    "#4th save is after 16 epochs, 5th save is after 35+28+36 +27+27+7epochs\n",
    "# torch.save(model.state_dict(),'deepset_dist_2conditions_5.pkl')\n",
    "torch.save(model.state_dict(),'pc_10_100_epoch100.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First save is at 31 epochs \n",
    "torch.save({\n",
    "    \n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss\n",
    "            \n",
    "            }, 'pc_10_100_epoch100.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load('pc_10_100_epoch100.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "# checkpoint = torch.load('pc_10_1000_w1_50.pkl')\n",
    "# model.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "def train(epoch, model, optimizer):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        batch = data.pos.reshape(-1, set_size, 3)\n",
    "        optimizer.zero_grad()\n",
    "        loss = L(model(batch), data.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss\n",
    "    print(total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader, model):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        batch = data.pos.reshape(-1, set_size, 3)\n",
    "        with torch.no_grad():\n",
    "            pred = model(batch).max(1)[1]\n",
    "        correct += pred.eq(data.y).sum().item()\n",
    "    return correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pc_classification_layers_10(nn.Module):\n",
    "    def __init__(self, inp_dim=256, out_dim=10):\n",
    "        super(pc_classification_layers_10, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "#             nn.Tanh(),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.Linear(inp_dim, 64),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.Linear(64, out_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(inp_dim, out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "class pc_classification_layers_40(nn.Module):\n",
    "    def __init__(self, inp_dim=256, out_dim=40):\n",
    "        super(pc_classification_layers_40, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "#             nn.Tanh(),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.Linear(inp_dim, 128),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.Linear(128, 40),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(inp_dim, out_dim),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = nn.Sequential(model, pc_classification_layers_10()).to(device)\n",
    "# model_new = nn.Sequential(model, pc_classification_layers_40()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_new = torch.optim.Adam(model_new.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "        train(epoch, model_new, optimizer_new)\n",
    "        test_acc = test(test_loader, model_new)\n",
    "        print('Epoch: {:03d}, Test: {:.4f}'.format(epoch, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tsne(loader, model, out_dim, epoch=0):\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        X = np.empty([len(loader.dataset), out_dim])\n",
    "        Y = np.empty(len(loader.dataset), dtype=int)\n",
    "\n",
    "        start = 0\n",
    "        for i, batch in enumerate(loader):\n",
    "            n_data = batch.pos.reshape(-1, set_size, 3).to(device)\n",
    "            encodings = model(n_data)\n",
    "            batch_size = n_data.shape[0]\n",
    "            X[start:start+batch_size] = encodings.detach().cpu().numpy()\n",
    "            Y[start:start+batch_size] = batch.y.cpu().numpy()\n",
    "            start += batch_size\n",
    "\n",
    "        X_2d = tsne.fit_transform(X)\n",
    "        target_ids = range(10)\n",
    "\n",
    "        plt.figure(figsize=(6, 5))\n",
    "        colors = 'r', 'g', 'b', 'c', 'm', 'y', 'k', 'brown', 'orange', 'purple'\n",
    "        for i, c, label in zip(target_ids, colors, target_ids):\n",
    "            plt.scatter(X_2d[Y == i, 0], X_2d[Y == i, 1], c=c, label=label)\n",
    "        plt.legend()\n",
    "        plt.savefig(\"tsne/morelayers_%d.png\" %epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(train_loader, model_new, 10)\n",
    "# plot_tsne(train_loader, model_new, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model_new.parameters():\n",
    "    param.requires_grad = True\n",
    "optimizer_full = torch.optim.Adam(model_new.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    train(epoch, model_new, optimizer_full)\n",
    "            \n",
    "    test_acc = test(test_loader, model_new)\n",
    "    print('Epoch: {:03d}, Test: {:.4f}'.format(epoch, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(train_loader, model_new, 10)\n",
    "# plot_tsne(train_loader, model_new, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSet_pc_full_10(nn.Module):\n",
    "\n",
    "    def __init__(self, d_dim=256, x_dim=3):\n",
    "        super(DeepSet_pc_full_10, self).__init__()\n",
    "        self.d_dim = d_dim\n",
    "        self.x_dim = x_dim\n",
    "\n",
    "        self.phi = nn.Sequential(\n",
    "          PermEqui1_max(self.x_dim, self.d_dim),\n",
    "          nn.Tanh(),\n",
    "          PermEqui1_max(self.d_dim, self.d_dim),\n",
    "          nn.Tanh(),\n",
    "          PermEqui1_max(self.d_dim, self.d_dim),\n",
    "          nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.ro = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(self.d_dim, self.d_dim),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.Linear(self.d_dim, 64),\n",
    "#             nn.Tanh(),\n",
    "#             nn.Dropout(p=0.5),\n",
    "#             nn.Linear(64, 10),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(self.d_dim, 10),\n",
    "        )\n",
    "        print(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        phi_output = self.phi(x)\n",
    "        sum_output, _ = phi_output.max(1)\n",
    "        ro_output = self.ro(sum_output)\n",
    "        return ro_output\n",
    "    \n",
    "class DeepSet_pc_full_40(nn.Module):\n",
    "\n",
    "    def __init__(self, d_dim=256, x_dim=3):\n",
    "        super(DeepSet_pc_full_40, self).__init__()\n",
    "        self.d_dim = d_dim\n",
    "        self.x_dim = x_dim\n",
    "\n",
    "        self.phi = nn.Sequential(\n",
    "          PermEqui1_max(self.x_dim, self.d_dim),\n",
    "          nn.Tanh(),\n",
    "          PermEqui1_max(self.d_dim, self.d_dim),\n",
    "          nn.Tanh(),\n",
    "          PermEqui1_max(self.d_dim, self.d_dim),\n",
    "          nn.Tanh(),\n",
    "        )\n",
    "\n",
    "        self.ro = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(self.d_dim, self.d_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(self.d_dim, 128),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(128, 40),\n",
    "        )\n",
    "        print(self)\n",
    "\n",
    "    def forward(self, x):\n",
    "        phi_output = self.phi(x)\n",
    "        sum_output, _ = phi_output.max(1)\n",
    "        ro_output = self.ro(sum_output)\n",
    "        return ro_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim=256\n",
    "model_base = DeepSet_pc_full_10(d_dim=dim).to(device)\n",
    "# model_base = DeepSet_pc_full_40(d_dim=dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_base = torch.optim.Adam(model_base.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "        train(epoch, model_base, optimizer_base)\n",
    "        test_acc = test(test_loader, model_base)\n",
    "        print('Epoch: {:03d}, Test: {:.4f}'.format(epoch, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(train_loader, model_base, 10)\n",
    "# plot_tsne(train_loader, model_base, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.rand(1,500,1)\n",
    "y = torch.rand(1,500,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = torch.distributions.normal.Normal(torch.tensor([0.0]), torch.tensor([1.0]))\n",
    "m1 = m.sample([500]).view(1,-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2 = m.sample([500]).view(1,-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = torch.distributions.normal.Normal(torch.tensor([.05]), torch.tensor([1.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1 = n.sample([500]).view(1,-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n2 = n.sample([500]).view(1,-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.distributions.normal.Normal(torch.tensor([.5]), torch.tensor([.75]))\n",
    "N1 = N.sample([500]).view(1,-1,1)\n",
    "N2 = N.sample([500]).view(1,-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(N1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(N2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.ones(500).view(1,-1,1)\n",
    "B = torch.zeros(500).view(1,-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinkhorn(A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.distributions.bernoulli.Bernoulli(torch.tensor([.3]))\n",
    "D = torch.distributions.bernoulli.Bernoulli(torch.tensor([.5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C1 = C.sample([500]).view(1,-1,1)\n",
    "D1 = D.sample([500]).view(1,-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(C1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(D1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sinkhorn(C1,D1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
